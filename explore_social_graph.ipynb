{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the biggest social graph\n",
    "\n",
    "In this tutorial, we are going to explore the biggest social graph on earth - Facebook.\n",
    "\n",
    "First, we'll need to setup the environment to be able to call Graph API. This include creating access token, downloading SDK and making some simple requests.\n",
    "\n",
    "Then, we want to verify that famous 6-degree of seperation theory.\n",
    "\n",
    "At last, we will do some NLP and ML work. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First query\n",
    "\n",
    "In this section, we will need to get an access token from Facebook, download the third party SDK and then using that SDK to get a list of friends of yours.\n",
    "\n",
    "you can find a detailed introduction to Graph API here https://developers.facebook.com/docs/graph-api/overview\n",
    "\n",
    "\n",
    "### Create Access Token\n",
    "Since our third-party SDK don't have the login ability, we will have to use the Graph API Explorer to get our token.\n",
    "\n",
    "1. open https://developers.facebook.com/tools/explorer\n",
    "2. Click on the **Get Token button** in the top right of the Explorer.\n",
    "3. Choose the option **Get User Access Token**.\n",
    "4. In the following dialog don't check any boxes, just click the blue **Get Access Token** button.\n",
    "5. You'll see a Facebook Login Dialog, click **OK** here to proceed.\n",
    "6. Now you could see your **Access Token** filled in the Explorer.\n",
    "\n",
    "\n",
    "**NOTE: This is a short-term token. It will expire in about 2 hours.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"EAACEdEose0cBAPR6ssxOepwIyOnj9NPpueuSrSJjEDpNLnlvQ3ZBIhy97EOe0puZC1ZA3BxoCElVYLNBmEZCiJv6KtTRlyJde1frVYWJ6vRx0ZBcua41Pbdq3XMCYiaFvxuHgfCj5OJfLIM9B8pfdTfX0FKybwhMugXPSeSf4APZCOnvqp4KQM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the first request\n",
    "\n",
    "The second thing you need to do is to download the sdk.\n",
    "\n",
    "In your terminal, use the following command to download the **facebook-sdk** using pip\n",
    "```\n",
    "pip install facebook-sdk\n",
    "```\n",
    "once this succeeds, you should be able to import the sdk in Python.\n",
    "\n",
    "Try the following code, it should not give you any warning.\n",
    "\n",
    "**NOTE: the facebook sdk only support version up to 2.7**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import facebook\n",
    "import json, requests, time, io, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "\n",
    "# graph = facebook.GraphAPI(access_token=ACCESS_TOKEN, version='2.7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the Graph\n",
    "\n",
    "Let's explain some basic concept here first.\n",
    "\n",
    "The Graph API is named after the idea of a 'social graph' - a representation of the information on Facebook composed of:\n",
    "\n",
    "* **nodes** - basically \"things\" such as a User, a Photo, a Page, a Comment\n",
    "* **edges** - the connections between those \"things\", such as a Page's Photos, or a Photo's Comments\n",
    "* **fields** - info about those \"things\", such as a person's birthday, or the name of a Page\n",
    "\n",
    "#### Nodes\n",
    "\n",
    "Each node has a unique **ID** which is used to access it via the Graph API. \n",
    "In the provided SDK, you could get any object by using the `get_object()` method.\n",
    "\n",
    "Below is an example for how to get your own User Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "me = graph.get_object(id='me') # NOTE, in graph api, 'me' is an alias of current token owner's id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edges\n",
    "\n",
    "Edges don't have an ID for it. It can only be used along with Nodes. for example, you could query for all your friends using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "friends = graph.get_connections(id='me', connection_name='friends')\n",
    "print friends['summary']['total_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a friends list\n",
    "\n",
    "As you can see in the above example, the output actually has pagination. So the final part of this secion is to handle that to get the full list of your friends' ids'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_connections(graph, id, connection_name):\n",
    "    results = []\n",
    "    after = ''\n",
    "    while True:\n",
    "        response = graph.get_connections(id=id, connection_name=connection_name,after=after,limit=100)\n",
    "        data = response['data']\n",
    "        results.extend([node['id'] for node in data])\n",
    "        if 'paging' not in response:\n",
    "            break;\n",
    "        if 'next' in response['paging']:\n",
    "            after = response['paging']['cursors']['after']\n",
    "        else:\n",
    "            break;\n",
    "    return results\n",
    "            \n",
    "\n",
    "def get_friends(graph, id):\n",
    "    return get_all_connections(graph, id, 'friends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(get_friends(graph, 'me'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Since Graph API V2.0, Facebook stops to provide a full list of friends of the user. that makes the original idea of this tutorial impossible.\n",
    "Now there is two possible solution here.\n",
    "1. Change to Twitter which I can get much more data than Facebook.\n",
    "2. Targeting public figures on Facebook like Tyler Swift. Those pages have publicly accessable posts. Then I can still do something like training a ML model to predict the type of the Page (Artist, Business, Places, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Page predictor\n",
    "\n",
    "In this section, we want to train a model to predict if a page represents an artist based on the publicly available posts.\n",
    "\n",
    "First, we need to use GraphAPI to collect all these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "#### Get the last 100 posts from the page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_posts(graph, id, limit=100):\n",
    "    print 'get', id\n",
    "    response = graph.get_connections(id=id, connection_name='posts', limit=limit)\n",
    "    df = pd.DataFrame(response['data'])\n",
    "#     print df.head()\n",
    "    if df.shape[0]:\n",
    "        return df.drop('story', axis=1, errors='ignore')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "posts_taylor_swift = get_posts(graph, '260735133964390')\n",
    "print (posts_taylor_swift.head())\n",
    "print len(posts_taylor_swift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search For All Public Pages\n",
    "The given sdk doesn't support search entpoint, we'll have to write it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GraphAPI:\n",
    "    def __init__(self, token):\n",
    "        self.token = token   \n",
    "    \n",
    "    def node_request(self, url, **payload):\n",
    "        base_url = \"https://graph.facebook.com/v2.7/\"\n",
    "        payload['access_token'] = self.token\n",
    "        r = requests.get(base_url + url, params=payload)\n",
    "        return json.loads(r.text)\n",
    "    \n",
    "    def connection_request(self, url, limit=None, **payload):\n",
    "        base_url = \"https://graph.facebook.com/v2.7/\"\n",
    "        payload['access_token'] = self.token\n",
    "        payload['limit'] = limit if limit <= 100 else 100\n",
    "        r = requests.get(base_url + url, params=payload)\n",
    "        response = json.loads(r.text)\n",
    "        \n",
    "        result = []\n",
    "        result.extend(response['data'])\n",
    "        while len(result) < limit:\n",
    "            if 'next' not in response['paging']:\n",
    "                break\n",
    "            response = json.loads(requests.get(response['paging']['next']).text)\n",
    "            result.extend(response['data'])\n",
    "#             time.sleep(1)\n",
    "        return pd.DataFrame(result)\n",
    "        \n",
    "api = GraphAPI(ACCESS_TOKEN)\n",
    "taylor = api.node_request('105799462785959', fields=['category'])\n",
    "artists = api.connection_request('search', type='page', q='Artist', limit=200)\n",
    "print taylor\n",
    "print len(artists)\n",
    "print artists.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the training and evaluation table\n",
    "Having these API, we now are able to collect all the data we need.\n",
    "First, we will make some search queries to get preferably random list of public pages.\n",
    "Then, we need to get all the posts from these pages. Also, we could get the page's category and use that as the source of truth to decide if this Page represents an Artist.\n",
    "\n",
    "Now, we could use the thing have to get a relatively small but enough dataset.\n",
    "Since Facebook doesn't provide an API to get all public pages, we have to use the `Search` query to get some of them with a hand picked list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     name\n",
      "id                                                       \n",
      "105799462785959                                    Artist\n",
      "354577571346409               Artist in Residence Program\n",
      "105453336153572                                  Artistic\n",
      "906740039387472                     Artistically Speaking\n",
      "348604348593352  Associazione Artisti di Strada di Milano\n",
      "                                                           message\n",
      "id                                                                \n",
      "283319135042170  اعزاءنا الطلبة \\nللاطلاع على القاعات الخاصة با...\n",
      "283319135042170  إعلان هام لجميع الطلبة\\nننوه للطلبة الأعزاء بض...\n",
      "283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n",
      "283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n",
      "283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n"
     ]
    }
   ],
   "source": [
    "# keyword_list = ['Artist', 'Musician', 'Trump', 'Hillary', 'PHP', 'Data Science', 'NBA', 'Alpine Ski', 'Scuba Diving', 'Computer']\n",
    "\n",
    "# def get_all_posts(graph, api, keyword_list, limit=1000):\n",
    "#     all_pages = []\n",
    "#     all_posts = {}\n",
    "#     for keyword in keyword_list:\n",
    "#         pages = api.connection_request('search', type='page', q=keyword, limit=limit)\n",
    "#         posts = {id: get_posts(graph, id) for id in pages['id']}\n",
    "#         all_pages.append(pages)\n",
    "#         all_posts.update(posts)\n",
    "#     return all_pages, all_posts\n",
    "\n",
    "\n",
    "with open('all_pages.csv', 'r') as csv:\n",
    "    df_pages = pd.DataFrame.from_csv(csv, encoding='UTF-8')\n",
    "    \n",
    "with open('all_posts.csv', 'r') as csv:\n",
    "    df_posts = pd.DataFrame.from_csv(csv, encoding='UTF-8')\n",
    "    \n",
    "# # all_pages, all_posts = get_all_posts(graph, api, keyword_list)\n",
    "# print len(df_pages)\n",
    "# print len(df_posts)\n",
    "print df_pages.head()\n",
    "print df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get each pages Category as the source of truth if the Page represents an artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Planner \t1\n",
      "Industrials \t5\n",
      "Author \t1\n",
      "Insurance Company \t13\n",
      "Performance Art \t2\n",
      "Arts & Entertainment \t123\n",
      "Entertainer \t1\n",
      "TV Show \t19\n",
      "Engineering/Construction \t5\n",
      "App Page \t8\n",
      "Community Organization \t26\n",
      "Outdoor Gear/Sporting Goods \t39\n",
      "Public Figure \t63\n",
      "Shopping/Retail \t115\n",
      "Science Website \t6\n",
      "TV Genre \t1\n",
      "Brand \t1\n",
      "Legal/Law \t5\n",
      "Technology Company \t3\n",
      "Medical & Health \t8\n",
      "Chemicals \t1\n",
      "Business/Economy Website \t1\n",
      "Podcast \t1\n",
      "Phone/Tablet \t4\n",
      "Political Organization \t93\n",
      "Record Label \t25\n",
      "Bar \t50\n",
      "Just For Fun \t5\n",
      "Athlete \t46\n",
      "Attractions/Things to Do \t28\n",
      "Magazine \t13\n",
      "Landmark \t7\n",
      "Organization \t47\n",
      "Sports Team \t38\n",
      "Scuba Diving Center \t3\n",
      "Profession \t15\n",
      "Hospital/Clinic \t1\n",
      "Ski & Snowboard Shop \t1\n",
      "Public Services & Government \t1\n",
      "Science & Engineering \t3\n",
      "Personal Blog \t5\n",
      "Sport \t6\n",
      "Event \t6\n",
      "Non-Governmental Organization (NGO) \t7\n",
      "News Personality \t2\n",
      "Spas/Beauty/Personal Care \t167\n",
      "Pet \t1\n",
      "TV Network \t3\n",
      "Computer Company \t1\n",
      "Interest \t20\n",
      "Internet/Software \t64\n",
      "Local Service \t1\n",
      "Amateur Sports Team \t16\n",
      "Movie Theater \t1\n",
      "Home Improvement \t5\n",
      "Song \t1\n",
      "Automotive \t16\n",
      "Musician \t2\n",
      "Business Person \t1\n",
      "Health/Medical/Pharmaceuticals \t7\n",
      "Website \t40\n",
      "School \t36\n",
      "Computers \t2\n",
      "Politician \t47\n",
      "Bank/Financial Service \t10\n",
      "Field of Study \t2\n",
      "University \t31\n",
      "Book Store \t4\n",
      "Museum/Art Gallery \t5\n",
      "Electronics Store \t1\n",
      "Automobiles and Parts \t2\n",
      "Education \t202\n",
      "Neighborhood \t2\n",
      "Recreation/Sports Website \t13\n",
      "Energy \t1\n",
      "Local Business \t1203\n",
      "Home Decor \t1\n",
      "Stadium, Arena & Sports Venue \t1\n",
      "Scientist \t1\n",
      "Retail and Consumer Merchandise \t20\n",
      "Lawyer \t2\n",
      "Real Estate \t31\n",
      "Grocery Store \t11\n",
      "Company \t112\n",
      "Journalist \t1\n",
      "Bank/Financial Institution \t4\n",
      "Doctor \t2\n",
      "Event Planning Service \t1\n",
      "Sports Venue & Stadium \t38\n",
      "Sports & Recreation \t216\n",
      "Book \t8\n",
      "Scuba Instructor \t1\n",
      "News/Media Website \t20\n",
      "Travel Company \t1\n",
      "Professional Service \t149\n",
      "Software Company \t1\n",
      "Transportation \t3\n",
      "Beauty \t2\n",
      "Government Organization \t8\n",
      "Cultural Center \t1\n",
      "Hotel \t24\n",
      "Biotechnology Company \t1\n",
      "Photographer \t1\n",
      "Video Game \t32\n",
      "Political Ideology \t1\n",
      "Sports Event \t34\n",
      "Religious Organization \t9\n",
      "Sports League \t73\n",
      "Local/Travel Website \t2\n",
      "Software \t10\n",
      "Actor/Director \t2\n",
      "Event Planner \t29\n",
      "Movie & Television Studio \t1\n",
      "Business Service \t51\n",
      "Library \t2\n",
      "Entrepreneur \t2\n",
      "Consulting Agency \t1\n",
      "Small Business \t13\n",
      "Cars \t1\n",
      "Musician/Band \t27\n",
      "Album \t1\n",
      "City \t2\n",
      "Reference Website \t2\n",
      "Society/Culture Website \t2\n",
      "Restaurant/Cafe \t53\n",
      "Movie \t41\n",
      "Writer \t1\n",
      "Product/Service \t24\n",
      "Other \t4\n",
      "Travel/Leisure \t67\n",
      "Cause \t6\n",
      "Farming/Agriculture \t2\n",
      "Games/Toys \t9\n",
      "Makeup Artist \t2\n",
      "Club \t25\n",
      "Pet Service \t4\n",
      "Internet Company \t1\n",
      "Computers/Internet Website \t10\n",
      "Non-Profit Organization \t82\n",
      "Media/News/Publishing \t57\n",
      "School Sports Team \t21\n",
      "Government Official \t2\n",
      "Public Places \t4\n",
      "Artist \t9\n",
      "Licensed Financial Representative \t3\n",
      "Consulting/Business Service \t37\n",
      "Community & Government \t4\n",
      "Airport \t1\n",
      "Concentration or Major \t23\n",
      "Biotechnology \t2\n",
      "TV Channel \t2\n",
      "Work Position \t42\n",
      "Coach \t4\n",
      "Education Website \t3\n",
      "Radio Station \t9\n",
      "Clothing \t8\n",
      "Government Website \t1\n",
      "Food/Beverages \t15\n",
      "Community \t235\n",
      "Musical Genre \t1\n",
      "Mining/Materials \t1\n",
      "Tours & Sightseeing \t25\n",
      "Performance Venue \t34\n",
      "Health/Beauty \t112\n",
      "Computers/Technology \t401\n"
     ]
    }
   ],
   "source": [
    "# def get_page_category(graph, page_ids):\n",
    "#     result = {}\n",
    "#     for i in range(0, len(page_ids), 50):\n",
    "#         pages = graph.get_objects(ids=page_ids[i:i+50], fields='category')\n",
    "#         result.update({id: page['category'] for id, page in pages.items() if 'category' in page or 'Unknown' })\n",
    "#     return [result[id] for id in page_ids if id in result or 'Unknown']\n",
    "\n",
    "# # df_pages = pd.concat(all_pages)\n",
    "# df_pages = df_pages.assign(category=get_page_category(graph, [str(id) for id in df_pages.index]))\n",
    "for k,v in df_pages.groupby('category').groups.items():\n",
    "    print k, '\\t', len(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     name  \\\n",
      "id                                                          \n",
      "105799462785959                                    Artist   \n",
      "354577571346409               Artist in Residence Program   \n",
      "105453336153572                                  Artistic   \n",
      "906740039387472                     Artistically Speaking   \n",
      "348604348593352  Associazione Artisti di Strada di Milano   \n",
      "\n",
      "                               category  \n",
      "id                                       \n",
      "105799462785959              Profession  \n",
      "354577571346409  Community Organization  \n",
      "105453336153572                Interest  \n",
      "906740039387472           Event Planner  \n",
      "348604348593352               Community  \n",
      "                                                           message\n",
      "id                                                                \n",
      "283319135042170  اعزاءنا الطلبة \\nللاطلاع على القاعات الخاصة با...\n",
      "283319135042170  إعلان هام لجميع الطلبة\\nننوه للطلبة الأعزاء بض...\n",
      "283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n",
      "283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n",
      "283319135042170  الجامعة تستقبل الدكتور سايمون غالبين المدير ال...\n"
     ]
    }
   ],
   "source": [
    "# print len(all_pages)\n",
    "# df = pd.concat(all_pages)\n",
    "# print df.head()\n",
    "# print len(df)\n",
    "\n",
    "# posts = [[id, posts.iloc[i]['message']] for id, posts in all_posts.items() if posts is not None for i in range(len(posts)) if 'message' in posts.iloc[i]]\n",
    "# pdf = pd.DataFrame(posts, columns=['id', 'message'])\n",
    "\n",
    "\n",
    "with open('all_pages.csv', 'w') as output:\n",
    "    df_pages.to_csv(output, encoding='UTF-8', index=True)\n",
    "    \n",
    "with open('all_posts.csv', 'w') as output:\n",
    "    df_posts.to_csv(output, encoding='UTF-8', index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "print df_pages.head()\n",
    "print df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                      name  \\\n",
      "0  105799462785959                                    Artist   \n",
      "1  354577571346409               Artist in Residence Program   \n",
      "2  105453336153572                                  Artistic   \n",
      "3  906740039387472                     Artistically Speaking   \n",
      "4  348604348593352  Associazione Artisti di Strada di Milano   \n",
      "\n",
      "                 category  \n",
      "0              Profession  \n",
      "1  Community Organization  \n",
      "2                Interest  \n",
      "3           Event Planner  \n",
      "4               Community  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Data pre processing\n",
    "Now that we've got the data, we need to do some pre-process first. This include\n",
    "\n",
    "1. remove all the pages with no posts\n",
    "2. remove all the non-English posts\n",
    "3. use a page's category to get the label column: 1 as IT related and -1 as non-IT related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305967\n",
      "                 id                                            message\n",
      "0  1507446969543009            GAME TIME!! Houston Rockets - NY Knicks\n",
      "1  1507446969543009  Dal caldo al freddo....da Manu Ginobili al Bar...\n",
      "2  1507446969543009   Tutto pronto per San Antonio Spurs - Miami Heat!\n",
      "3  1507446969543009  Vista la giornata nuvolosa, relax e shopping s...\n",
      "4  1507446969543009                                       EXTRA GAME!!\n",
      "150326\n"
     ]
    }
   ],
   "source": [
    "def pre_process_data(df_posts):\n",
    "    df = df_posts[pd.notnull(df_posts['message'])]\n",
    "    \n",
    "    def is_ascii(message):\n",
    "        try:\n",
    "            message.decode('ascii')\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    df = df[df['message'].apply(is_ascii)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print len(df_posts)\n",
    "posts = pre_process_data(df_posts)\n",
    "print posts.head()\n",
    "print len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP to get feature \n",
    "Now that we have all our row data with some pre-process. we could use Nature Language Process(NLP) technique to create our features to train the Machine Learning model.\n",
    "\n",
    "In this section, we need to do convert all the post content into a set of tokens for each page. for each token, we need to:\n",
    "1. convert all the token to lowercase\n",
    "2. convert all the token to their lemmatized form. This will remove all the Non-English content.\n",
    "3. all words with punctuation should be processed as follows: (a) Apostrophe of the form `'s` should be ignored. (b)All other apostrophe should be ignored. (c) Break the word at all other punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'s\", '').replace(\"'\", '')\n",
    "    replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(replace_punctuation)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            res = lemmatizer.lemmatize(token)\n",
    "            result.append(res)\n",
    "        except:\n",
    "            continue\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[[12, 3, 4, 5], [12, 23, 23]]\n",
      "   id          value\n",
      "0   1  [12, 3, 4, 5]\n",
      "1   2   [12, 23, 23]\n"
     ]
    }
   ],
   "source": [
    "dict = {1:[12,3,4,5], 2:[12,23,23]}\n",
    "print dict.keys()\n",
    "print dict.values()\n",
    "df = pd.DataFrame().assign(id=dict.keys(), value=dict.values())\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of this function, we could now generate a list of tokens for each Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                            message  \\\n",
      "0  314534308659695  [skonczona, praca, kosatattoo, w90, wygojona, ...   \n",
      "1      65409974260  [2, lake, huron, shipwreck, lost, since, the, ...   \n",
      "2  258308394333462  [it, is, about, time, this, poop, is, held, ac...   \n",
      "3      78367162355  [congratulation, to, tomeka, reid, http, www, ...   \n",
      "4  712517218826239  [lol, literally, sleeping, with, the, fish, wh...   \n",
      "\n",
      "                                                name  \\\n",
      "0             Studio Tatuażu  SPEAK IN COLOR by Kosa   \n",
      "1                                       Total Diving   \n",
      "2                            Hillary For Prison 2016   \n",
      "3  AACM - Association for the Advancement of Crea...   \n",
      "4                              Scuba Diving Globally   \n",
      "\n",
      "                      category  \n",
      "0               Local Business  \n",
      "1  Outdoor Gear/Sporting Goods  \n",
      "2       Political Organization  \n",
      "3      Non-Profit Organization  \n",
      "4               Travel/Leisure  \n"
     ]
    }
   ],
   "source": [
    "def generate_tokens(df_pages, df_posts):\n",
    "    result = {}\n",
    "    for id, indices in df_posts.groupby('id').groups.items():\n",
    "        token = [process(str(df_posts.iloc[index]['message'])) for index in indices]\n",
    "        token = np.concatenate(token)\n",
    "        result[id] = token\n",
    "    \n",
    "    data = pd.merge(pd.DataFrame().assign(id=result.keys(), message=result.values()), df_pages, on='id', how='left')\n",
    "    return data\n",
    "\n",
    "data = generate_tokens(df_pages, posts)\n",
    "print data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When doing prediction based on natural languange, it's almost certain that you don't need that many possible words.\n",
    "Some of them are very popular which add no value to our model, like stopwords. Others are too rare that are most likely to be typos.\n",
    "In the NLTK package, they provide a list of stopwords we could borrow. In the following section, we need to get a list of rare words. Rare words are defined as words only occured once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91644\n"
     ]
    }
   ],
   "source": [
    "def get_rare_words(data):\n",
    "    \"\"\" use the word count information across all posts in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        data: pd.DataFrame: the output of generate_tokens() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    token_list = [token for tokens in data['message'] for token in tokens]\n",
    "    counter = Counter(token_list)\n",
    "    return [k for k,v in counter.iteritems() if v == 1]\n",
    "\n",
    "rare_words = get_rare_words(data)\n",
    "print len(rare_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could create a feature matrix for each page using `sklearn.feature_extraction.text.TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3619, 72898) 3619\n"
     ]
    }
   ],
   "source": [
    "def create_features(data, rare_words):\n",
    "    \"\"\" creates the feature matrix using the Page posts\n",
    "    Inputs:\n",
    "        data: pd.DataFrame: Page posts collected above\n",
    "        rare_words: list(str): the output of get_rare_words() function\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(rare_words)\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords)\n",
    "    transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "    # get frequency counts (sparse) matrix\n",
    "    all_tokens = [' '.join(tokens) for tokens in data['message']]\n",
    "    freq_matrix = vectorizer.fit_transform(all_tokens)\n",
    "    return (vectorizer, freq_matrix)\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "(tfidf, X) = create_features(data, rare_words)\n",
    "print X.shape, len(data)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_labels(data):\n",
    "    it_category = ['Computers/Technology', 'Computer Company', 'Internet/Software', 'Computers', 'Software',\n",
    "                  'Internet Company', 'Computers/Internet Website']\n",
    "    return np.array([1 if category in it_category else 0 for category in data['category']])\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "y = create_labels(data)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_classifier(X, y, kernel='linear'):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features_and_labels()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_features_and_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [best|linear|poly|rbf|sigmoid]\n",
    "                    if 'best' is supplied, reset the kernel parameter to the value you have determined to be the best\n",
    "    Outputs:\n",
    "        sklearn.svm.classes.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    clf = sklearn.svm.SVC(kernel=kernel)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "N = len(y)\n",
    "X_train = X[:N/2]\n",
    "y_train = y[:N/2]\n",
    "X_eval = X[N/2:]\n",
    "y_eval = y[N/2:]\n",
    "classifier = learn_classifier(X_train, y_train, 'linear')\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9773355445\n",
      "0.882872928177\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    result = [classifier.predict(X_validation[i]) == y_validation[i] for i in range(X_validation.shape[0])]\n",
    "    return float(sum(result)) / X_validation.shape[0]\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "accuracy = evaluate_classifier(classifier, X_train, y_train)\n",
    "print accuracy # should give 0.954850271708\n",
    "\n",
    "accuracy = evaluate_classifier(classifier, X_eval, y_eval)\n",
    "print accuracy # should give 0.954850271708\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
